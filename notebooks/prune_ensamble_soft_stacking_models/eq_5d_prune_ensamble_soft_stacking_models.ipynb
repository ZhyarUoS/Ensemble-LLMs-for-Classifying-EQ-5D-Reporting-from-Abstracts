{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TUqvQ7ywB6N",
        "outputId": "59a4cd53-568f-4e53-f460-3d8d0999555a"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjTnoPjBwI71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from google.colab import ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hKpu3jKwN86"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"drive/MyDrive/eq_5d/eq-5d-200-records.csv\"\n",
        "SAVE_DIR = \"drive/MyDrive/eq_5d/eq_5d_soft_stacking\"\n",
        "USE_FEW_SHOT = True\n",
        "\n",
        "models = [\n",
        " \"google/gemini-2.5-pro\",\n",
        " \"google/gemma-3-12b\",\n",
        " \"google/gemma-3-27b\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUq_GDHSwQb9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03drBadxwSY9"
      },
      "outputs": [],
      "source": [
        "def make_zero_shot_prompt(abstract):\n",
        "    return f\"\"\"\n",
        "You are a biomedical text classification expert.\n",
        "\n",
        "Task: Determine whether the following clinical study abstract provides **explicit evidence** that the EQ-5D instrument (or a variant like EQ-5D-3L, EQ-5D-5L, EuroQol-5D) was actually **used** in the study's methods or results.\n",
        "\n",
        "Answer format (exactly one line):\n",
        "Prediction: [Yes/No]\n",
        "Confidence: [number between 0 and 100]\n",
        "\n",
        "Abstract:\n",
        "\\\"\\\"\\\"{abstract.strip()}\\\"\\\"\\\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZGTk6iXwVc6"
      },
      "outputs": [],
      "source": [
        "def make_few_shot_prompt(abstract, examples):\n",
        "    prompt = \"\"\"You are a biomedical text classification expert.\n",
        "\n",
        "Task: Determine whether the following clinical study abstract provides **explicit evidence** that the EQ-5D instrument (or a variant like EQ-5D-3L, EQ-5D-5L, EuroQol-5D) was actually **used** in the study's methods or results.\n",
        "\n",
        "Answer format (exactly one line):\n",
        "Prediction: [Yes/No]\n",
        "Confidence: [number between 0 and 100]\n",
        "\n",
        "Here are examples:\n",
        "\n",
        "\"\"\"\n",
        "    for _, ex in examples.iterrows():\n",
        "        label = \"Yes\" if ex[\"Label\"] == 1 else \"No\"\n",
        "        conf = \"90\" if ex[\"Label\"] == 1 else \"85\"\n",
        "        prompt += f\"\"\"Abstract:\n",
        "\\\"\\\"\\\"{ex['Abstract'].strip()}\\\"\\\"\\\"\n",
        "Prediction: {label}\n",
        "Confidence: {conf}\n",
        "\n",
        "\"\"\"\n",
        "    prompt += f\"\"\"\\nNow classify this new abstract:\n",
        "\n",
        "Abstract:\n",
        "\\\"\\\"\\\"{abstract.strip()}\\\"\\\"\\\"\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def parse_prediction(response):\n",
        "    pred, conf = \"No\", 50\n",
        "    try:\n",
        "        match_pred = re.search(r\"Prediction:\\s*(Yes|No)\", response, re.I)\n",
        "        match_conf = re.search(r\"Confidence:\\s*(\\d+)\", response)\n",
        "        if match_pred:\n",
        "            pred = match_pred.group(1).capitalize()\n",
        "        if match_conf:\n",
        "            conf = int(match_conf.group(1))\n",
        "    except:\n",
        "        pass\n",
        "    return pred, conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOsjIqI2wcIW"
      },
      "outputs": [],
      "source": [
        "if USE_FEW_SHOT:\n",
        "    pos_examples = df[df[\"Label\"] == 1].sample(20, random_state=42) # 121\n",
        "    neg_examples = df[df[\"Label\"] == 0].sample(20, random_state=42) # 79\n",
        "    few_shot_examples = pd.concat([pos_examples, neg_examples])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3343eda8",
        "outputId": "033cee72-063b-430a-e1e3-a6d580099f3e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "    print(f\"Created directory: {SAVE_DIR}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {SAVE_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv-UswjcwgfG",
        "outputId": "b6c171de-41b0-494f-8e27-1709ceff36a6"
      },
      "outputs": [],
      "source": [
        "all_results = []\n",
        "\n",
        "for model_name in models:\n",
        "    model_results = []\n",
        "    print(f\"\\n Running predictions for {model_name}\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if USE_FEW_SHOT:\n",
        "            prompt = make_few_shot_prompt(row[\"Abstract\"], few_shot_examples)\n",
        "        else:\n",
        "            prompt = make_zero_shot_prompt(row[\"Abstract\"])\n",
        "\n",
        "        try:\n",
        "            response = ai.generate_text(\n",
        "                prompt=prompt,\n",
        "                model_name=model_name\n",
        "            )\n",
        "            pred, conf = parse_prediction(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {model_name}, doc {row['No']}: {e}\")\n",
        "            pred, conf = \"No\", 50\n",
        "\n",
        "        model_results.append({\n",
        "            \"No\": row[\"No\"],\n",
        "            \"True_Label\": row[\"Label\"],\n",
        "            \"Prediction\": 1 if pred == \"Yes\" else 0,\n",
        "            \"Confidence\": conf\n",
        "        })\n",
        "\n",
        "        time.sleep(1.0)\n",
        "\n",
        "    model_df = pd.DataFrame(model_results)\n",
        "    model_df.to_csv(f\"{SAVE_DIR}/preds_{model_name.replace('/','_')}_pruned_2.csv\", index=False)\n",
        "    all_results.append((model_name, model_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc8CqiEiwjM5",
        "outputId": "4adb0e03-75ac-4b80-cfbc-c5b2dd3b007b"
      },
      "outputs": [],
      "source": [
        "model_weights = {}\n",
        "model_scores = {}  \n",
        "\n",
        "for model_name, model_df in all_results:\n",
        "    print(f\"\\n Results for {model_name}\")\n",
        "    report = classification_report(model_df[\"True_Label\"], model_df[\"Prediction\"], output_dict=True)\n",
        "    print(classification_report(model_df[\"True_Label\"], model_df[\"Prediction\"]))\n",
        "    f1 = report[\"weighted avg\"][\"f1-score\"]\n",
        "    model_weights[model_name] = f1\n",
        "    model_scores[model_name] = f1  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-i_d2UYwls4",
        "outputId": "22696719-2aa5-4d38-8c87-dbb51d6fb461"
      },
      "outputs": [],
      "source": [
        "def weighted_vote(row, model_dfs, model_weights):\n",
        "    yes_score, no_score = 0, 0\n",
        "    for model_name, model_df in model_dfs:\n",
        "        pred = model_df.loc[model_df[\"No\"] == row[\"No\"], \"Prediction\"].values[0]\n",
        "        conf = model_df.loc[model_df[\"No\"] == row[\"No\"], \"Confidence\"].values[0]\n",
        "        weight = model_weights.get(model_name, 1.0)\n",
        "        if pred == 1:\n",
        "            yes_score += weight * (conf / 100)\n",
        "        else:\n",
        "            no_score += weight * (conf / 100)\n",
        "    return 1 if yes_score >= no_score else 0\n",
        "\n",
        "ensemble_preds = []\n",
        "for _, row in df.iterrows():\n",
        "    final_pred = weighted_vote(row, all_results, model_weights)\n",
        "    ensemble_preds.append({\n",
        "        \"No\": row[\"No\"],\n",
        "        \"True_Label\": row[\"Label\"],\n",
        "        \"Ensemble_Pred\": final_pred\n",
        "    })\n",
        "\n",
        "ensemble_df = pd.DataFrame(ensemble_preds)\n",
        "ensemble_df.to_csv(f\"{SAVE_DIR}/ensemble_predictions_pruned.csv\", index=False)\n",
        "\n",
        "print(\"\\n Pruned Ensemble Results\")\n",
        "report_ens = classification_report(ensemble_df[\"True_Label\"], ensemble_df[\"Ensemble_Pred\"], output_dict=True)\n",
        "print(classification_report(ensemble_df[\"True_Label\"], ensemble_df[\"Ensemble_Pred\"]))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(ensemble_df[\"True_Label\"], ensemble_df[\"Ensemble_Pred\"]))\n",
        "\n",
        "model_scores[\"Pruned_Ensemble\"] = report_ens[\"weighted avg\"][\"f1-score\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUZ85DusuBYB",
        "outputId": "e86ccae6-d1b0-4f1c-adae-5ee22d745b12"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"\\n===== STACKING WITH SOFT PROBABILITIES (CV) =====\")\n",
        "\n",
        "features = pd.DataFrame({\"No\": df[\"No\"], \"True_Label\": df[\"Label\"]})\n",
        "\n",
        "for model_name, model_df in all_results:\n",
        "    probs = []\n",
        "    for p, c in zip(model_df[\"Prediction\"], model_df[\"Confidence\"]):\n",
        "        if p == 1:   # Yes\n",
        "            prob_yes = c / 100\n",
        "        else:        # No\n",
        "            prob_yes = 1 - (c / 100)\n",
        "        probs.append(prob_yes)\n",
        "\n",
        "    features[f\"{model_name}_prob_yes\"] = probs\n",
        "    features[f\"{model_name}_conf\"] = model_df[\"Confidence\"].values\n",
        "\n",
        "X = features.drop(columns=[\"No\", \"True_Label\"])\n",
        "y = features[\"True_Label\"]\n",
        "\n",
        "meta_clf = LogisticRegression(max_iter=1000)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "y_pred = cross_val_predict(meta_clf, X, y, cv=cv)\n",
        "\n",
        "stacking_soft_results = pd.DataFrame({\n",
        "    \"No\": features[\"No\"],\n",
        "    \"True_Label\": y,\n",
        "    \"Stacking_Soft_Pred\": y_pred\n",
        "})\n",
        "stacking_soft_results.to_csv(f\"{SAVE_DIR}/stacking_predictions_pruned_soft.csv\", index=False)\n",
        "\n",
        "# Evaluate\n",
        "print(\"\\n Stacking (Soft Probabilities, 5-fold CV)\")\n",
        "print(classification_report(y, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y, y_pred))\n",
        "\n",
        "meta_clf.fit(X, y)\n",
        "coef = meta_clf.coef_[0]\n",
        "importance = pd.Series(coef, index=X.columns).sort_values(ascending=False)\n",
        "print(\"\\n Top 10 Most Influential Features in Soft Stacking Model:\")\n",
        "print(importance.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "V9doSLNnuFFP",
        "outputId": "cc8eaa16-3c0c-4efc-e21e-808f8bdaae82"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "labels = list(model_scores.keys())\n",
        "values = list(model_scores.values())\n",
        "\n",
        "colors = plt.cm.tab10(np.linspace(0, 1, len(labels)))\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "bars = plt.bar(labels, values, color=colors)\n",
        "\n",
        "for bar, val in zip(bars, values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2,\n",
        "             bar.get_height() + 0.02,\n",
        "             f\"{val*100:.1f}%\",\n",
        "             ha='center', va='bottom', fontsize=10, fontweight=\"bold\")\n",
        "\n",
        "plt.ylabel(\"Weighted F1 Score\")\n",
        "plt.title(\"Model Performance Comparison (Pruned Models, Ensemble, Stacking)\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.ylim(0,1.1)  #\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{SAVE_DIR}/performance_comparison_1.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et3Qr0h74nCJ",
        "outputId": "fbe98d21-ff81-49ff-b248-2e8c0736b8a4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "df1 = pd.read_csv(f\"{SAVE_DIR}/preds_google_gemini-2.5-pro.csv\")\n",
        "df2 = pd.read_csv(f\"{SAVE_DIR}/preds_google_gemma-3-12b.csv\")\n",
        "df3 = pd.read_csv(f\"{SAVE_DIR}/preds_google_gemma-3-27b.csv\")\n",
        "\n",
        "merged = df1[[\"No\", \"True_Label\"]].copy()\n",
        "\n",
        "for i, df in enumerate([df1, df2, df3], start=1):\n",
        "    merged[f\"Prediction_{i}\"] = df[\"Prediction\"]\n",
        "    merged[f\"Confidence_{i}\"] = df[\"Confidence\"]\n",
        "\n",
        "final_preds = []\n",
        "\n",
        "for _, row in merged.iterrows():\n",
        "    preds = [row[\"Prediction_1\"], row[\"Prediction_2\"], row[\"Prediction_3\"]]\n",
        "    confs = [row[\"Confidence_1\"], row[\"Confidence_2\"], row[\"Confidence_3\"]]\n",
        "\n",
        "    counts = pd.Series(preds).value_counts()\n",
        "    top_count = counts.max()\n",
        "    majority_classes = counts[counts == top_count].index.tolist()\n",
        "\n",
        "    if len(majority_classes) == 1:\n",
        "        final_preds.append(majority_classes[0])\n",
        "    else:\n",
        "        best_idx = max(\n",
        "            range(3),\n",
        "            key=lambda i: confs[i] if preds[i] in majority_classes else -1\n",
        "        )\n",
        "        final_preds.append(preds[best_idx])\n",
        "\n",
        "merged[\"Final_Prediction\"] = final_preds\n",
        "\n",
        "merged.to_csv(\"final_results.csv\", index=False)\n",
        "\n",
        "print(classification_report(merged[\"True_Label\"], merged[\"Final_Prediction\"]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
