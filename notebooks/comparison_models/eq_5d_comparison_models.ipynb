{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TUqvQ7ywB6N",
        "outputId": "887c0e44-16ab-4f88-9c03-8f3eb2de33eb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjTnoPjBwI71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hKpu3jKwN86"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"drive/MyDrive/eq_5d/eq-5d-200-records.csv\"\n",
        "SAVE_DIR = \"drive/MyDrive/eq_5d\"\n",
        "USE_FEW_SHOT = True\n",
        "\n",
        "models = [\n",
        " 'google/gemini-2.0-flash',\n",
        " 'google/gemini-2.0-flash-lite',\n",
        " 'google/gemini-2.5-flash',\n",
        " 'google/gemini-2.5-flash-lite',\n",
        " 'google/gemini-2.5-pro',\n",
        " 'google/gemma-3-12b',\n",
        " 'google/gemma-3-1b',\n",
        " 'google/gemma-3-27b',\n",
        " 'google/gemma-3-4b'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUq_GDHSwQb9"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03drBadxwSY9"
      },
      "outputs": [],
      "source": [
        "def make_zero_shot_prompt(abstract):\n",
        "    return f\"\"\"\n",
        "You are a biomedical text classification expert.\n",
        "\n",
        "Task: Determine whether the following clinical study abstract provides **explicit evidence** that the EQ-5D instrument (or a variant like EQ-5D-3L, EQ-5D-5L, EuroQol-5D) was actually **used** in the study's methods or results.\n",
        "\n",
        "Answer format (exactly one line):\n",
        "Prediction: [Yes/No]\n",
        "Confidence: [number between 0 and 100]\n",
        "\n",
        "Abstract:\n",
        "\\\"\\\"\\\"{abstract.strip()}\\\"\\\"\\\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZGTk6iXwVc6"
      },
      "outputs": [],
      "source": [
        "def make_few_shot_prompt(abstract, examples):\n",
        "    prompt = \"\"\"You are a biomedical text classification expert.\n",
        "\n",
        "Task: Determine whether the following clinical study abstract provides **explicit evidence** that the EQ-5D instrument (or a variant like EQ-5D-3L, EQ-5D-5L, EuroQol-5D) was actually **used** in the study's methods or results.\n",
        "\n",
        "Answer format (exactly one line):\n",
        "Prediction: [Yes/No]\n",
        "Confidence: [number between 0 and 100]\n",
        "\n",
        "Here are examples:\n",
        "\n",
        "\"\"\"\n",
        "    for _, ex in examples.iterrows():\n",
        "        label = \"Yes\" if ex[\"Label\"] == 1 else \"No\"\n",
        "        conf = \"90\" if ex[\"Label\"] == 1 else \"85\"\n",
        "        prompt += f\"\"\"Abstract:\n",
        "\\\"\\\"\\\"{ex['Abstract'].strip()}\\\"\\\"\\\"\n",
        "Prediction: {label}\n",
        "Confidence: {conf}\n",
        "\n",
        "\"\"\"\n",
        "    prompt += f\"\"\"\\nNow classify this new abstract:\n",
        "\n",
        "Abstract:\n",
        "\\\"\\\"\\\"{abstract.strip()}\\\"\\\"\\\"\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def parse_prediction(response):\n",
        "    pred, conf = \"No\", 50\n",
        "    try:\n",
        "        match_pred = re.search(r\"Prediction:\\s*(Yes|No)\", response, re.I)\n",
        "        match_conf = re.search(r\"Confidence:\\s*(\\d+)\", response)\n",
        "        if match_pred:\n",
        "            pred = match_pred.group(1).capitalize()\n",
        "        if match_conf:\n",
        "            conf = int(match_conf.group(1))\n",
        "    except:\n",
        "        pass\n",
        "    return pred, conf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOsjIqI2wcIW"
      },
      "outputs": [],
      "source": [
        "if USE_FEW_SHOT:\n",
        "    pos_examples = df[df[\"Label\"] == 1].sample(20, random_state=42)\n",
        "    neg_examples = df[df[\"Label\"] == 0].sample(20, random_state=42)\n",
        "    few_shot_examples = pd.concat([pos_examples, neg_examples])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv-UswjcwgfG",
        "outputId": "8b577b74-2d68-4c3e-b519-1e9d692b4408"
      },
      "outputs": [],
      "source": [
        "all_results = []\n",
        "\n",
        "for model_name in models:\n",
        "    model_results = []\n",
        "    print(f\"\\nRunning predictions for {model_name}\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if USE_FEW_SHOT:\n",
        "            prompt = make_few_shot_prompt(row[\"Abstract\"], few_shot_examples)\n",
        "        else:\n",
        "            prompt = make_zero_shot_prompt(row[\"Abstract\"])\n",
        "\n",
        "        try:\n",
        "            response = ai.generate_text(\n",
        "                prompt=prompt,\n",
        "                model_name=model_name\n",
        "            )\n",
        "            pred, conf = parse_prediction(response)\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {model_name}, doc {row['No']}: {e}\")\n",
        "            pred, conf = \"No\", 50\n",
        "\n",
        "        model_results.append({\n",
        "            \"No\": row[\"No\"],\n",
        "            \"True_Label\": row[\"Label\"],\n",
        "            \"Prediction\": 1 if pred == \"Yes\" else 0,\n",
        "            \"Confidence\": conf\n",
        "        })\n",
        "\n",
        "        time.sleep(1.0)\n",
        "\n",
        "    # Save per-model predictions\n",
        "    model_df = pd.DataFrame(model_results)\n",
        "    model_df.to_csv(f\"{SAVE_DIR}/preds_{model_name.replace('/','_')}.csv\", index=False)\n",
        "    all_results.append((model_name, model_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc8CqiEiwjM5",
        "outputId": "5f026703-072e-42a1-e550-d9b71e6e5747"
      },
      "outputs": [],
      "source": [
        "model_weights = {}\n",
        "\n",
        "for model_name, model_df in all_results:\n",
        "    print(f\"\\n Results for {model_name}\")\n",
        "    print(classification_report(model_df[\"True_Label\"], model_df[\"Prediction\"]))\n",
        "    f1 = classification_report(\n",
        "        model_df[\"True_Label\"], model_df[\"Prediction\"], output_dict=True\n",
        "    )[\"weighted avg\"][\"f1-score\"]\n",
        "    model_weights[model_name] = f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-i_d2UYwls4"
      },
      "outputs": [],
      "source": [
        "def weighted_vote(row, model_dfs, model_weights):\n",
        "    yes_score, no_score = 0, 0\n",
        "    for model_name, model_df in model_dfs:\n",
        "        pred = model_df.loc[model_df[\"No\"] == row[\"No\"], \"Prediction\"].values[0]\n",
        "        conf = model_df.loc[model_df[\"No\"] == row[\"No\"], \"Confidence\"].values[0]\n",
        "        weight = model_weights.get(model_name, 1.0)\n",
        "        if pred == 1:\n",
        "            yes_score += weight * (conf / 100)\n",
        "        else:\n",
        "            no_score += weight * (conf / 100)\n",
        "    return 1 if yes_score >= no_score else 0\n",
        "\n",
        "ensemble_preds = []\n",
        "for idx, row in df.iterrows():\n",
        "    final_pred = weighted_vote(row, all_results, model_weights)\n",
        "    ensemble_preds.append({\n",
        "        \"No\": row[\"No\"],\n",
        "        \"True_Label\": row[\"Label\"],\n",
        "        \"Ensemble_Pred\": final_pred\n",
        "    })\n",
        "\n",
        "ensemble_df = pd.DataFrame(ensemble_preds)\n",
        "ensemble_df.to_csv(f\"{SAVE_DIR}/ensemble_predictions.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn70awEnwngv",
        "outputId": "94e17abb-eb57-4666-c4a9-650db374f5b8"
      },
      "outputs": [],
      "source": [
        "print(\"\\n Ensemble Results\")\n",
        "print(classification_report(ensemble_df[\"True_Label\"], ensemble_df[\"Ensemble_Pred\"]))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(ensemble_df[\"True_Label\"], ensemble_df[\"Ensemble_Pred\"]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
