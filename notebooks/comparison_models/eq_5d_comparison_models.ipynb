{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TUqvQ7ywB6N",
        "outputId": "887c0e44-16ab-4f88-9c03-8f3eb2de33eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from google.colab import ai"
      ],
      "metadata": {
        "id": "MjTnoPjBwI71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"drive/MyDrive/eq_5d/eq-5d-200-records.csv\"\n",
        "SAVE_DIR = \"drive/MyDrive/eq_5d\"\n",
        "USE_FEW_SHOT = True\n",
        "\n",
        "models = [\n",
        " 'google/gemini-2.0-flash',\n",
        " 'google/gemini-2.0-flash-lite',\n",
        " 'google/gemini-2.5-flash',\n",
        " 'google/gemini-2.5-flash-lite',\n",
        " 'google/gemini-2.5-pro',\n",
        " 'google/gemma-3-12b',\n",
        " 'google/gemma-3-1b',\n",
        " 'google/gemma-3-27b',\n",
        " 'google/gemma-3-4b'\n",
        "]"
      ],
      "metadata": {
        "id": "7hKpu3jKwN86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(DATA_PATH)\n"
      ],
      "metadata": {
        "id": "lUq_GDHSwQb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_zero_shot_prompt(abstract):\n",
        "    return f\"\"\"\n",
        "You are a biomedical text classification expert.\n",
        "\n",
        "Task: Determine whether the following clinical study abstract provides **explicit evidence** that the EQ-5D instrument (or a variant like EQ-5D-3L, EQ-5D-5L, EuroQol-5D) was actually **used** in the study's methods or results.\n",
        "\n",
        "Answer format (exactly one line):\n",
        "Prediction: [Yes/No]\n",
        "Confidence: [number between 0 and 100]\n",
        "\n",
        "Abstract:\n",
        "\\\"\\\"\\\"{abstract.strip()}\\\"\\\"\\\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "03drBadxwSY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_few_shot_prompt(abstract, examples):\n",
        "    prompt = \"\"\"You are a biomedical text classification expert.\n",
        "\n",
        "Task: Determine whether the following clinical study abstract provides **explicit evidence** that the EQ-5D instrument (or a variant like EQ-5D-3L, EQ-5D-5L, EuroQol-5D) was actually **used** in the study's methods or results.\n",
        "\n",
        "Answer format (exactly one line):\n",
        "Prediction: [Yes/No]\n",
        "Confidence: [number between 0 and 100]\n",
        "\n",
        "Here are examples:\n",
        "\n",
        "\"\"\"\n",
        "    for _, ex in examples.iterrows():\n",
        "        label = \"Yes\" if ex[\"Label\"] == 1 else \"No\"\n",
        "        conf = \"90\" if ex[\"Label\"] == 1 else \"85\"\n",
        "        prompt += f\"\"\"Abstract:\n",
        "\\\"\\\"\\\"{ex['Abstract'].strip()}\\\"\\\"\\\"\n",
        "Prediction: {label}\n",
        "Confidence: {conf}\n",
        "\n",
        "\"\"\"\n",
        "    prompt += f\"\"\"\\nNow classify this new abstract:\n",
        "\n",
        "Abstract:\n",
        "\\\"\\\"\\\"{abstract.strip()}\\\"\\\"\\\"\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def parse_prediction(response):\n",
        "    pred, conf = \"No\", 50\n",
        "    try:\n",
        "        match_pred = re.search(r\"Prediction:\\s*(Yes|No)\", response, re.I)\n",
        "        match_conf = re.search(r\"Confidence:\\s*(\\d+)\", response)\n",
        "        if match_pred:\n",
        "            pred = match_pred.group(1).capitalize()\n",
        "        if match_conf:\n",
        "            conf = int(match_conf.group(1))\n",
        "    except:\n",
        "        pass\n",
        "    return pred, conf"
      ],
      "metadata": {
        "id": "9ZGTk6iXwVc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_FEW_SHOT:\n",
        "    pos_examples = df[df[\"Label\"] == 1].sample(20, random_state=42)\n",
        "    neg_examples = df[df[\"Label\"] == 0].sample(20, random_state=42)\n",
        "    few_shot_examples = pd.concat([pos_examples, neg_examples])\n"
      ],
      "metadata": {
        "id": "nOsjIqI2wcIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = []\n",
        "\n",
        "for model_name in models:\n",
        "    model_results = []\n",
        "    print(f\"\\nðŸš€ Running predictions for {model_name}\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        if USE_FEW_SHOT:\n",
        "            prompt = make_few_shot_prompt(row[\"Abstract\"], few_shot_examples)\n",
        "        else:\n",
        "            prompt = make_zero_shot_prompt(row[\"Abstract\"])\n",
        "\n",
        "        try:\n",
        "            response = ai.generate_text(\n",
        "                prompt=prompt,\n",
        "                model_name=model_name\n",
        "            )\n",
        "            pred, conf = parse_prediction(response)\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error with {model_name}, doc {row['No']}: {e}\")\n",
        "            pred, conf = \"No\", 50\n",
        "\n",
        "        model_results.append({\n",
        "            \"No\": row[\"No\"],\n",
        "            \"True_Label\": row[\"Label\"],\n",
        "            \"Prediction\": 1 if pred == \"Yes\" else 0,\n",
        "            \"Confidence\": conf\n",
        "        })\n",
        "\n",
        "        time.sleep(1.0)\n",
        "\n",
        "    # Save per-model predictions\n",
        "    model_df = pd.DataFrame(model_results)\n",
        "    model_df.to_csv(f\"{SAVE_DIR}/preds_{model_name.replace('/','_')}.csv\", index=False)\n",
        "    all_results.append((model_name, model_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv-UswjcwgfG",
        "outputId": "8b577b74-2d68-4c3e-b519-1e9d692b4408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Running predictions for google/gemini-2.0-flash\n",
            "\n",
            "ðŸš€ Running predictions for google/gemini-2.0-flash-lite\n",
            "\n",
            "ðŸš€ Running predictions for google/gemini-2.5-flash\n",
            "\n",
            "ðŸš€ Running predictions for google/gemini-2.5-flash-lite\n",
            "\n",
            "ðŸš€ Running predictions for google/gemini-2.5-pro\n",
            "\n",
            "ðŸš€ Running predictions for google/gemma-3-12b\n",
            "\n",
            "ðŸš€ Running predictions for google/gemma-3-1b\n",
            "\n",
            "ðŸš€ Running predictions for google/gemma-3-27b\n",
            "\n",
            "ðŸš€ Running predictions for google/gemma-3-4b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_weights = {}\n",
        "\n",
        "for model_name, model_df in all_results:\n",
        "    print(f\"\\nðŸ“Š Results for {model_name}\")\n",
        "    print(classification_report(model_df[\"True_Label\"], model_df[\"Prediction\"]))\n",
        "    f1 = classification_report(\n",
        "        model_df[\"True_Label\"], model_df[\"Prediction\"], output_dict=True\n",
        "    )[\"weighted avg\"][\"f1-score\"]\n",
        "    model_weights[model_name] = f1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc8CqiEiwjM5",
        "outputId": "5f026703-072e-42a1-e550-d9b71e6e5747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Results for google/gemini-2.0-flash\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.18      0.29        79\n",
            "           1       0.64      0.98      0.78       121\n",
            "\n",
            "    accuracy                           0.66       200\n",
            "   macro avg       0.73      0.58      0.53       200\n",
            "weighted avg       0.72      0.66      0.58       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemini-2.0-flash-lite\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.06      0.12        79\n",
            "           1       0.62      0.99      0.76       121\n",
            "\n",
            "    accuracy                           0.62       200\n",
            "   macro avg       0.73      0.53      0.44       200\n",
            "weighted avg       0.70      0.62      0.51       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemini-2.5-flash\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.19      0.30        79\n",
            "           1       0.64      0.94      0.76       121\n",
            "\n",
            "    accuracy                           0.65       200\n",
            "   macro avg       0.66      0.57      0.53       200\n",
            "weighted avg       0.66      0.65      0.58       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemini-2.5-flash-lite\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.24      0.37        79\n",
            "           1       0.66      0.95      0.78       121\n",
            "\n",
            "    accuracy                           0.67       200\n",
            "   macro avg       0.71      0.60      0.57       200\n",
            "weighted avg       0.70      0.67      0.61       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemini-2.5-pro\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.44      0.57        79\n",
            "           1       0.72      0.93      0.81       121\n",
            "\n",
            "    accuracy                           0.73       200\n",
            "   macro avg       0.76      0.68      0.69       200\n",
            "weighted avg       0.75      0.73      0.71       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemma-3-12b\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.80      0.64        79\n",
            "           1       0.81      0.55      0.66       121\n",
            "\n",
            "    accuracy                           0.65       200\n",
            "   macro avg       0.67      0.68      0.65       200\n",
            "weighted avg       0.70      0.65      0.65       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemma-3-1b\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.22      0.28        79\n",
            "           1       0.60      0.78      0.68       121\n",
            "\n",
            "    accuracy                           0.56       200\n",
            "   macro avg       0.49      0.50      0.48       200\n",
            "weighted avg       0.52      0.56      0.52       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemma-3-27b\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.44      0.51        79\n",
            "           1       0.69      0.81      0.75       121\n",
            "\n",
            "    accuracy                           0.67       200\n",
            "   macro avg       0.65      0.63      0.63       200\n",
            "weighted avg       0.66      0.67      0.65       200\n",
            "\n",
            "\n",
            "ðŸ“Š Results for google/gemma-3-4b\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.30      0.37        79\n",
            "           1       0.63      0.79      0.70       121\n",
            "\n",
            "    accuracy                           0.59       200\n",
            "   macro avg       0.56      0.54      0.54       200\n",
            "weighted avg       0.57      0.59      0.57       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_vote(row, model_dfs, model_weights):\n",
        "    yes_score, no_score = 0, 0\n",
        "    for model_name, model_df in model_dfs:\n",
        "        pred = model_df.loc[model_df[\"No\"] == row[\"No\"], \"Prediction\"].values[0]\n",
        "        conf = model_df.loc[model_df[\"No\"] == row[\"No\"], \"Confidence\"].values[0]\n",
        "        weight = model_weights.get(model_name, 1.0)\n",
        "        if pred == 1:\n",
        "            yes_score += weight * (conf / 100)\n",
        "        else:\n",
        "            no_score += weight * (conf / 100)\n",
        "    return 1 if yes_score >= no_score else 0\n",
        "\n",
        "ensemble_preds = []\n",
        "for idx, row in df.iterrows():\n",
        "    final_pred = weighted_vote(row, all_results, model_weights)\n",
        "    ensemble_preds.append({\n",
        "        \"No\": row[\"No\"],\n",
        "        \"True_Label\": row[\"Label\"],\n",
        "        \"Ensemble_Pred\": final_pred\n",
        "    })\n",
        "\n",
        "ensemble_df = pd.DataFrame(ensemble_preds)\n",
        "ensemble_df.to_csv(f\"{SAVE_DIR}/ensemble_predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "H-i_d2UYwls4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nðŸ“Š Ensemble Results\")\n",
        "print(classification_report(ensemble_df[\"True_Label\"], ensemble_df[\"Ensemble_Pred\"]))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(ensemble_df[\"True_Label\"], ensemble_df[\"Ensemble_Pred\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn70awEnwngv",
        "outputId": "94e17abb-eb57-4666-c4a9-650db374f5b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Š Ensemble Results\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.20      0.32        79\n",
            "           1       0.65      0.97      0.78       121\n",
            "\n",
            "    accuracy                           0.67       200\n",
            "   macro avg       0.73      0.58      0.55       200\n",
            "weighted avg       0.71      0.67      0.60       200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 16  63]\n",
            " [  4 117]]\n"
          ]
        }
      ]
    }
  ]
}